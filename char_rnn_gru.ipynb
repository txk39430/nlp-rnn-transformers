{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21db0ffe",
   "metadata": {},
   "source": [
    "# Tiny Character-level RNN Text Generator (PyTorch, GRU default)\n",
    "\n",
    "**Goal:** Train a tiny character-level model to predict the next character given previous characters.\n",
    "\n",
    "**Pipeline:** Embedding → RNN (Vanilla RNN / GRU / LSTM) → Linear → Softmax over characters  \n",
    "**Training:** Teacher forcing, Cross-Entropy Loss, Adam Optimizer\n",
    "\n",
    "This notebook includes:\n",
    "- A tiny **toy corpus** for quick tests.\n",
    "- Either **auto-download** a small public-domain text (e.g., *Pride and Prejudice* excerpt) **or upload your own `.txt`**.\n",
    "- Switchable RNN type: `\"gru\"` (default), `\"lstm\"`, or `\"rnn\"`.\n",
    "- Training & validation **loss curves**.\n",
    "- Temperature-controlled **sampling** (τ = 0.7, 1.0, 1.2).\n",
    "- A short **reflection** cell.\n",
    "\n",
    "> Tip: Start with the toy text and a small number of epochs to verify everything runs, then expand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Setup: imports and device\n",
    "import math, os, io, sys, random, textwrap, time, urllib.request\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Configuration (feel free to tweak)\n",
    "model_type = \"gru\"  #@param [\"gru\", \"lstm\", \"rnn\"] {allow-input: true}\n",
    "embed_size = 128    #@param {type:\"integer\"}\n",
    "hidden_size = 128   #@param {type:\"integer\"}\n",
    "num_layers = 1      #@param {type:\"integer\"}\n",
    "dropout = 0.0       #@param {type:\"number\"}\n",
    "\n",
    "block_size = 100    #@param {type:\"integer\"}  # sequence length (50–100 suggested)\n",
    "batch_size = 64     #@param {type:\"integer\"}\n",
    "epochs = 10         #@param {type:\"integer\"}\n",
    "\n",
    "learning_rate = 1e-3  #@param {type:\"number\"}\n",
    "\n",
    "# Sampling\n",
    "sample_start = \"The \"  #@param {type:\"string\"}\n",
    "gen_length = 300       #@param {type:\"integer\"}  # 200–400 suggested\n",
    "temperatures = [0.7, 1.0, 1.2]  #@param\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model: {model_type}, hidden={hidden_size}, layers={num_layers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26871e2",
   "metadata": {},
   "source": [
    "## Data: choose one path\n",
    "- **Toy text** (quick smoke test).\n",
    "- **Auto-download** a ~100–200 KB excerpt from a public-domain novel (requires internet).\n",
    "- **Upload your own `.txt`** (works in Colab/Jupyter).\n",
    "\n",
    "> You can run multiple cells below and switch which `text_source` you pass into the preprocessing cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Option A: Tiny Toy Text (no internet needed)\n",
    "toy_text = \"\"\"hello help helium hero\n",
    "hello helper helpful hello\n",
    "helium helps heroes hello\n",
    "\"\"\"\n",
    "print(toy_text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687fd0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Option B: Auto-download a small public-domain excerpt (requires internet)\n",
    "# We'll attempt to download an excerpt of Pride and Prejudice (public domain).\n",
    "# If internet is blocked, this will fail gracefully and you can upload instead.\n",
    "\n",
    "download_text = \"\"\n",
    "try:\n",
    "    url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"  # Pride and Prejudice\n",
    "    with urllib.request.urlopen(url, timeout=20) as resp:\n",
    "        raw = resp.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    # Keep a slice (~150 KB) to stay small and quick to train\n",
    "    download_text = raw[:160_000]\n",
    "    print(\"Downloaded text length:\", len(download_text))\n",
    "    print(download_text[:500])\n",
    "except Exception as e:\n",
    "    print(\"Download failed:\", e)\n",
    "    print(\"You can use the toy text or upload your own in the next cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Option C: Upload a .txt file (Colab: click the file chooser)\n",
    "# If you're in Google Colab, this will prompt a file chooser.\n",
    "# In Jupyter, you can place a file in the working dir and set upload_text_path to it.\n",
    "\n",
    "upload_text = \"\"\n",
    "upload_text_path = \"\"  #@param {type:\"string\"}\n",
    "try:\n",
    "    # Colab helper (ignored if not present)\n",
    "    from google.colab import files  # type: ignore\n",
    "    print(\"Colab detected. Use the chooser dialog to upload a .txt file (cancel to skip).\")\n",
    "    uploaded = files.upload()  # opens dialog\n",
    "    if uploaded:\n",
    "        fname = list(uploaded.keys())[0]\n",
    "        upload_text = uploaded[fname].decode(\"utf-8\", errors=\"ignore\")\n",
    "        print(\"Uploaded:\", fname, \"| length:\", len(upload_text))\n",
    "except Exception as _:\n",
    "    # Fallback: read from a path if user set it\n",
    "    if upload_text_path and os.path.exists(upload_text_path):\n",
    "        with open(upload_text_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            upload_text = f.read()\n",
    "        print(\"Read from path:\", upload_text_path, \"| length:\", len(upload_text))\n",
    "    else:\n",
    "        print(\"No upload used. Set `upload_text_path` or run in Colab to choose a file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Preprocess: choose which text source to use and build vocabulary\n",
    "# Choose one of: toy_text, download_text, upload_text\n",
    "text_source = \"download_text\"  #@param [\"toy_text\", \"download_text\", \"upload_text\"] {allow-input: true}\n",
    "\n",
    "text = globals().get(text_source, \"\")\n",
    "if not text:\n",
    "    raise ValueError(f\"Selected source '{text_source}' is empty. Try another source or re-run the earlier cells.\")\n",
    "\n",
    "# Normalize newlines; keep characters as-is otherwise for a char-level model\n",
    "text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "# Train/val split\n",
    "split = int(0.9 * len(text))\n",
    "train_text = text[:split]\n",
    "val_text = text[split:]\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(s: str):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ix: list):\n",
    "    return ''.join(itos[i] for i in ix)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Sample vocab:\", chars[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Dataset & DataLoaders (teacher forcing via shifted targets)\n",
    "import torch.utils.data as tud\n",
    "\n",
    "class CharDataset(tud.Dataset):\n",
    "    def __init__(self, data_str: str, block_size: int):\n",
    "        self.data = torch.tensor(encode(data_str), dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.block_size]\n",
    "        y = self.data[idx + 1 : idx + 1 + self.block_size]  # shift by one (teacher forcing)\n",
    "        return x, y\n",
    "\n",
    "train_ds = CharDataset(train_text, block_size)\n",
    "val_ds   = CharDataset(val_text, block_size)\n",
    "\n",
    "train_loader = tud.DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader   = tud.DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5bd8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Model: Embedding → (RNN/GRU/LSTM) → Linear\n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=128, model_type=\"gru\", num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type.lower()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        if self.model_type == \"gru\":\n",
    "            self.rnn = nn.GRU(embed_size, hidden_size, num_layers=num_layers, dropout=dropout if num_layers>1 else 0.0, batch_first=True)\n",
    "        elif self.model_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, dropout=dropout if num_layers>1 else 0.0, batch_first=True)\n",
    "        elif self.model_type == \"rnn\":\n",
    "            self.rnn = nn.RNN(embed_size, hidden_size, nonlinearity=\"tanh\", num_layers=num_layers, dropout=dropout if num_layers>1 else 0.0, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'gru', 'lstm', or 'rnn'\")\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)  # (B, T, E)\n",
    "        out, hidden = self.rnn(x, hidden)  # (B, T, H)\n",
    "        logits = self.fc(out)  # (B, T, V)\n",
    "        return logits, hidden\n",
    "\n",
    "model = CharModel(vocab_size, embed_size, hidden_size, model_type, num_layers, dropout).to(device)\n",
    "sum(p.numel() for p in model.parameters()), model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Train\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "def run_epoch(loader, train_mode=True):\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        logits, _ = model(x)\n",
    "        # reshape for CE loss\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "\n",
    "        if train_mode:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.numel()\n",
    "        total_tokens += x.numel()\n",
    "\n",
    "    return total_loss / max(total_tokens, 1)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss = run_epoch(train_loader, train_mode=True)\n",
    "    va_loss = run_epoch(val_loader, train_mode=False)\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(va_loss)\n",
    "    print(f\"Epoch {epoch:02d} | train CE: {tr_loss:.4f} | val CE: {va_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Plot training & validation loss\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Training/Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d454a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Sampling (temperature)\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, start_text=\"The \", length=300, temperature=1.0):\n",
    "    model.eval()\n",
    "    if not start_text:\n",
    "        start_text = random.choice(chars)\n",
    "    input_ids = torch.tensor([encode(start_text)], dtype=torch.long, device=device)\n",
    "    hidden = None\n",
    "    generated = list(start_text)\n",
    "\n",
    "    for _ in range(length):\n",
    "        logits, hidden = model(input_ids, hidden)\n",
    "        last_logits = logits[:, -1, :] / max(1e-6, temperature)\n",
    "        probs = F.softmax(last_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # sample\n",
    "        next_char = itos[next_id.item()]\n",
    "        generated.append(next_char)\n",
    "        input_ids = next_id  # feed the sampled char\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "for t in temperatures:\n",
    "    print(\"\\n\" + \"=\"*20 + f\" Temperature {t} \" + \"=\"*20)\n",
    "    print(sample(model, start_text=sample_start, length=gen_length, temperature=t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626ac64",
   "metadata": {},
   "source": [
    "## Reflection (3–5 sentences)\n",
    "\n",
    "- **Sequence length:** Longer `block_size` lets the model capture longer dependencies but can make optimization harder and slower; with small data it can overfit/repeat.\n",
    "- **Hidden size:** Larger `hidden_size` increases capacity and can improve fluency, but may overfit on small corpora and train slower (more parameters).\n",
    "- **Temperature:** Lower τ (e.g., 0.7) yields safer, more repetitive text; higher τ (e.g., 1.2) increases diversity but can become incoherent.\n",
    "- **Teacher forcing:** Stabilizes training by using ground-truth previous chars, but at inference the model must cope without them (train–test mismatch).\n",
    "\n",
    "_Add your observations from your runs here._"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
